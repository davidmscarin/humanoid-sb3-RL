{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from stable_baselines3 import PPO, TD3\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Agent Training and Testing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d3c984415348fea7bbcfc55de3680e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting seed 1\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing phase\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9896b0c7644400ae8456b413db28a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting seed 2\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing phase\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5369acfb5a034fd2b3ac8a6641b43353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting seed 3\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rewards_over_seeds = []\n",
    "\n",
    "for seed in [1,2,3]:\n",
    "    print(f\"Starting seed {seed}\")\n",
    "    steps_in_seed = 0\n",
    "    env_train = gym.make(\"Humanoid-v4\")\n",
    "    env_test = gym.make(\"Humanoid-v4\", healthy_z_range=(0.9,2.0))\n",
    "    # Record each episode reward\n",
    "    wrapped_env_train = gym.wrappers.RecordEpisodeStatistics(env_train)\n",
    "    wrapped_env_test = gym.wrappers.RecordEpisodeStatistics(env_test)\n",
    "\n",
    "    # Set seed\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    print(f\"Starting training\")\n",
    "    agent = PPO('MlpPolicy', wrapped_env_train, verbose=0, tensorboard_log=\"./logs_default\")\n",
    "    agent.learn(total_timesteps=2000000,progress_bar=True, tb_log_name=\"PPO_\"+str(seed), reset_num_timesteps=False)\n",
    "    agent.save(\"agent_default_\"+str(seed))\n",
    "    del agent\n",
    "    env_train.close()\n",
    "\n",
    "    print(f\"Starting testing phase\")\n",
    "    agent = PPO.load(\"agent_default_\"+str(seed), env = wrapped_env_test)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "\n",
    "    for episode in range(1000):\n",
    "        obs, info = wrapped_env_test.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _states = agent.predict(obs)\n",
    "            obs, reward, terminated, truncated, info = wrapped_env_test.step(action)\n",
    "            done = terminated or truncated\n",
    "            #wrapped_env_test.render()\n",
    "        \n",
    "        reward_over_episodes.append(wrapped_env_test.return_queue[-1][0])\n",
    "    \n",
    "    rewards_over_seeds.append(reward_over_episodes)\n",
    "    env_test.close()\n",
    "    print()\n",
    "\n",
    "# Store the rewards for further analysis\n",
    "with open('rewards_default', 'wb') as f:  \n",
    "    pickle.dump(rewards_over_seeds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed Agent Training and Testing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting seed 3\n",
      "Starting 1st phase of training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c950ca73c7407a8496fceda49be35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2nd phase of training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2065f6e6b24e2eb598a551d3a23faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing phase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rewards_over_seeds = []\n",
    "\n",
    "for seed in [1,2,3]:\n",
    "    print(f\"Starting seed {seed}\")\n",
    "    steps_in_seed = 0\n",
    "    env_train_1 = gym.make(\"Humanoid-v4\", healthy_reward = 10, forward_reward_weight = 0.625,  healthy_z_range=(0.9,2.0))\n",
    "    env_train_2 = gym.make(\"Humanoid-v4\", healthy_reward = 2.5, forward_reward_weight = 2.5, healthy_z_range=(0.9,2.0))\n",
    "    env_test = gym.make(\"Humanoid-v4\", healthy_z_range=(0.9,2.0))\n",
    "    # Record each episode reward\n",
    "    wrapped_env_train_1 = gym.wrappers.RecordEpisodeStatistics(env_train_1)\n",
    "    wrapped_env_train_2 = gym.wrappers.RecordEpisodeStatistics(env_train_2)\n",
    "    wrapped_env_test = gym.wrappers.RecordEpisodeStatistics(env_test)\n",
    "\n",
    "    # Set seed\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    print(f\"Starting 1st phase of training\")\n",
    "    agent = PPO('MlpPolicy', wrapped_env_train_1, verbose=0, tensorboard_log=\"./logs_changed\")\n",
    "    agent.learn(total_timesteps=700000,progress_bar=True, tb_log_name=\"PPO_1_\"+str(seed), reset_num_timesteps=False)\n",
    "    agent.save(\"agent_changed_1_\"+str(seed))\n",
    "    del agent\n",
    "    env_train_1.close()\n",
    "    \n",
    "    print(f\"Starting 2nd phase of training\")\n",
    "    agent = PPO.load(\"agent_changed_1_\"+str(seed), env = wrapped_env_train_2)\n",
    "    agent.learn(total_timesteps=1300000,progress_bar=True, tb_log_name=\"PPO_2_\"+str(seed), reset_num_timesteps=False)\n",
    "    agent.save(\"agent_changed_2_\"+str(seed))\n",
    "    del agent\n",
    "    env_train_2.close()\n",
    "\n",
    "    print(f\"Starting testing phase\")\n",
    "    agent = PPO.load(\"agent_changed_2_\"+str(seed), env = wrapped_env_test)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "\n",
    "    for episode in range(1000):\n",
    "        obs, info = wrapped_env_test.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _states = agent.predict(obs)\n",
    "            obs, reward, terminated, truncated, info = wrapped_env_test.step(action)\n",
    "            done = terminated or truncated\n",
    "            #wrapped_env_test.render()\n",
    "        \n",
    "        reward_over_episodes.append(wrapped_env_test.return_queue[-1][0])\n",
    "    \n",
    "    rewards_over_seeds.append(reward_over_episodes)\n",
    "    env_test.close()\n",
    "    print()\n",
    "\n",
    "# Store the rewards for further analysis\n",
    "with open('rewards_changed', 'wb') as f:  \n",
    "    pickle.dump(rewards_over_seeds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch How a Trained Agent Behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make(\"Humanoid-v4\", render_mode = \"human\", healthy_z_range=(0.9,2.0))\n",
    "wrapped_env_test = gym.wrappers.RecordEpisodeStatistics(env_test)\n",
    "agent = PPO.load(\"agents/agent_changed_1_1\", env = wrapped_env_test)\n",
    "seed = 42\n",
    "\n",
    "for episode in range(10):\n",
    "    obs, info = wrapped_env_test.reset(seed=seed)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _states = agent.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = wrapped_env_test.step(action)\n",
    "        done = terminated or truncated\n",
    "        wrapped_env_test.render()\n",
    "            \n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
